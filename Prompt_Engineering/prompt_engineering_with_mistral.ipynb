{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajabhupati/AI-Session/blob/main/Prompt_Engineering/prompt_engineering_with_mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWHrP7mWsOlG"
      },
      "source": [
        "# Prompt Engineering with Open Source LLMs\n",
        "This notebook demonstrates zero-shot, few-shot, and instruction prompting using `Mistral-7B-Instruct` on Hugging Face.\n",
        "\n",
        "*Note: Ollama is not supported in Google Colab as it requires a local Docker runtime.*"
      ],
      "id": "zWHrP7mWsOlG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G1qy1HnsOlH"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers accelerate bitsandbytes"
      ],
      "id": "8G1qy1HnsOlH"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb058U3FyYvy",
        "outputId": "406d61a7-0db3-4e54-bbed-7850e9ef58ad"
      },
      "id": "yb058U3FyYvy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sat May  3 11:15:10 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0             34W /   70W |    7172MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "# Suppress specific UserWarnings related to computation\n",
        "warnings.filterwarnings(\"ignore\", message=\"Input type into Linear4bit\")"
      ],
      "metadata": {
        "id": "dGiUaNxI24F7"
      },
      "id": "dGiUaNxI24F7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Suppress info-level messages in transformers\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "hmYdTFOa3JpZ"
      },
      "id": "hmYdTFOa3JpZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "a4iIDByQsOlH",
        "outputId": "3754b277-f920-43c8-a949-608689273036"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-daf4827ddb78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuggingface_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4380\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "# Replace 'your_token_here' with an actual token temporarily for testing\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Directly set the Hugging Face token in your code (not ideal for real use due to exposure)\n",
        "huggingface_token = userdata.get('HUGGINGFACE_TOKEN')\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# Ensure token is accessible\n",
        "if not huggingface_token:\n",
        "    raise ValueError(\"HUGGINGFACE_TOKEN is not set. Please ensure it's configured correctly.\")\n",
        "\n",
        "# Load model and tokenizer using token\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=huggingface_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        "    token=huggingface_token\n",
        ")\n",
        "\n",
        "def generate(prompt, max_tokens=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=True, temperature=0.7)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "id": "a4iIDByQsOlH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNdDBnbxsOlH"
      },
      "source": [
        "## Zero-Shot Prompting"
      ],
      "id": "nNdDBnbxsOlH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfhaPAEqsOlH",
        "outputId": "840da9f1-2fe0-42e0-f0c7-33a2b7099b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate this sentence to French: 'I love machine learning.'\n",
            "\n",
            "'Je adore l'apprentissage de la machine.'\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Translate this sentence to French: 'I love machine learning.'\"\n",
        "print(generate(prompt))"
      ],
      "id": "JfhaPAEqsOlH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOEtDUP9sOlH"
      },
      "source": [
        "## Few-Shot Prompting"
      ],
      "id": "wOEtDUP9sOlH"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ekC2Cl4gsEDR"
      },
      "id": "ekC2Cl4gsEDR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt, max_tokens=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=True, temperature=0.7)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "mplJxO-N1s0h"
      },
      "id": "mplJxO-N1s0h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYMMNcC7sOlH",
        "outputId": "9893591f-8b74-40b2-f278-887da2754491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct the grammatical errors in the following sentences:\n",
            "Incorrect: The cat don't like water.\n",
            "Correct: The cat doesn't like water.\n",
            "\n",
            "Incorrect: She go to store yesterday.\n",
            "Correct: She went to the store yesterday.\n",
            "\n",
            "Incorrect: He are playing in the park.\n",
            "Correct: He is playing in the park.\n",
            "\n",
            "Incorrect: They will be arriving at 5 pm.\n",
            "Correct: They will be arriving at 5 pm.\n",
            "\n",
            "Incorrect: I have saw the movie before.\n",
            "Correct: I have seen the movie before.\n"
          ]
        }
      ],
      "source": [
        "few_shot_prompt_grammar = '''Correct the grammatical errors in the following sentences:\n",
        "Incorrect: The cat don't like water.\n",
        "Correct: The cat doesn't like water.\n",
        "\n",
        "Incorrect: She go to store yesterday.\n",
        "Correct: She went to the store yesterday.\n",
        "\n",
        "Incorrect: He are playing in the park.\n",
        "Correct:'''\n",
        "print(generate_response(few_shot_prompt_grammar))"
      ],
      "id": "lYMMNcC7sOlH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwBR2Hs0sOlI"
      },
      "source": [
        "## Instruction Prompting"
      ],
      "id": "hwBR2Hs0sOlI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPIQ3EBysOlI",
        "outputId": "6a07983d-3d8f-4f51-fcce-8ed0b7aac693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful assistant. Summarize the following paragraph:\n",
            "\n",
            "Machine learning is a branch of artificial intelligence that focuses on building systems capable of learning from data and making predictions or decisions without being explicitly programmed. It encompasses a variety of techniques, including supervised learning, where models are trained on labeled datasets, and unsupervised learning, where they identify patterns or structures in data without preexisting labels. Deep learning, a subset of machine learning, uses neural networks with many layers to model complex patterns and relationships in data. Machine learning is transforming industries by providing powerful tools for analyzing large datasets, automating tasks, optimizing processes, and enhancing decision-making. Applications range from computer vision and natural language processing to recommendation systems and predictive analytics. As data continues to grow exponentially, the demand for effective machine learning models increases, driving research into new architectures, training methods, and interpretability to bridge the gap between theoretical advancements and real-world applications.\n",
            "\n",
            "Summary: Machine learning is a subfield of AI that uses data to build models for prediction or decision-making. Supervised and unsupervised learning techniques are used, with deep learning being a subset. The field is transforming industries and has numerous applications. As data grows, there is a demand for better models, leading to research into new methods and interpretability.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the expanded instruction prompt with the larger paragraph\n",
        "instruction_prompt = (\n",
        "    \"You are a helpful assistant. Summarize the following paragraph:\\n\\n\"\n",
        "    \"Machine learning is a branch of artificial intelligence that focuses on building systems capable of \"\n",
        "    \"learning from data and making predictions or decisions without being explicitly programmed. It encompasses \"\n",
        "    \"a variety of techniques, including supervised learning, where models are trained on labeled datasets, and \"\n",
        "    \"unsupervised learning, where they identify patterns or structures in data without preexisting labels. \"\n",
        "    \"Deep learning, a subset of machine learning, uses neural networks with many layers to model complex \"\n",
        "    \"patterns and relationships in data. Machine learning is transforming industries by providing powerful \"\n",
        "    \"tools for analyzing large datasets, automating tasks, optimizing processes, and enhancing decision-making. \"\n",
        "    \"Applications range from computer vision and natural language processing to recommendation systems and \"\n",
        "    \"predictive analytics. As data continues to grow exponentially, the demand for effective machine learning \"\n",
        "    \"models increases, driving research into new architectures, training methods, and interpretability to \"\n",
        "    \"bridge the gap between theoretical advancements and real-world applications.\\n\\nSummary:\"\n",
        ")\n",
        "\n",
        "# Generate the summary using the defined function\n",
        "print(generate(instruction_prompt))"
      ],
      "id": "GPIQ3EBysOlI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVzNG2ElsOlI"
      },
      "source": [
        "## Prompt Variation (Soft Prompt Tuning)"
      ],
      "id": "UVzNG2ElsOlI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv7PtvaPsOlI",
        "outputId": "bec93be3-5e7e-45f7-ecf2-62437d8f9c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please act as a French translator. What is the French for: 'I love machine learning'?\n",
            "\n",
            "I love machine learning\n",
            "\n",
            "\"Je aime l'apprentissage de la machine\"\n"
          ]
        }
      ],
      "source": [
        "tweaked_prompt = \"Please act as a French translator. What is the French for: 'I love machine learning'?\"\n",
        "print(generate(tweaked_prompt))"
      ],
      "id": "Wv7PtvaPsOlI"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}